<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
"http://www.w3.org/TR/html4/loose.dtd">
<html><head><title></title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
</head>
<body>
<div style="page-break-before:always; page-break-after:always"><div><p>DiffProtect: Generate Adversarial Examples with Diffusion Models for Facial<br/>Privacy Protection<br/></p>
<p>Jiang Liu*, Chun Pong Lau*, Rama Chellappa<br/>Johns Hopkins University, *Equal Contribution<br/></p>
<p>{jiangliu, clau13, rchella4}@jhu.edu<br/></p>
<p>Abstract<br/>The increasingly pervasive facial recognition (FR) sys-<br/></p>
<p>tems raise serious concerns about personal privacy, espe-<br/>cially for billions of users who have publicly shared their<br/>photos on social media. Several attempts have been made<br/>to protect individuals from being identified by unautho-<br/>rized FR systems utilizing adversarial attacks to generate<br/>encrypted face images. However, existing methods suffer<br/>from poor visual quality or low attack success rates, which<br/>limit their utility. Recently, diffusion models have achieved<br/>tremendous success in image generation. In this work, we<br/>ask: can diffusion models be used to generate adversar-<br/>ial examples to improve both visual quality and attack per-<br/>formance? We propose DiffProtect, which utilizes a diffu-<br/>sion autoencoder to generate semantically meaningful per-<br/>turbations on FR systems. Extensive experiments demon-<br/>strate that DiffProtect produces more natural-looking en-<br/>crypted images than state-of-the-art methods while achiev-<br/>ing significantly higher attack success rates, e.g., 24.5%<br/>and 25.1% absolute improvements on the CelebA-HQ and<br/>FFHQ datasets.<br/>1. Introduction<br/></p>
<p>The rise of deep neural networks has enabled the tremen-<br/>dous success of facial recognition (FR) systems [5, 43, 15].<br/>However, the widely deployed FR systems also pose a huge<br/>threat to personal privacy as billions of users have publicly<br/>shared their photos on social media. Through large-scale<br/>social media photo analysis, FR systems can be used for<br/>detecting user relationships [46], stalking victims [47],<br/>stealing identities [32], and performing massive govern-<br/>ment surveillance [42, 17, 34]. It is urgent to develop facial<br/>privacy protection techniques to protect individuals from<br/>unauthorized FR systems.<br/></p>
<p>Recently, several works [3, 58, 20] proposed to use ad-<br/>versarial attacks to generate encrypted face images and pro-<br/>tect users from being identified by FR systems. However,<br/>existing attacks on FR systems [3, 20, 27, 45, 59, 58, 61]<br/>often suffer from poor visual quality, especially noise-<br/></p>
<p>(a) Noise-based: TIP-IM&#160;&#160;<br/></p>
<p>(d) Diffusion-based: DiffProtect&#160;<b>(Ours)<br/></b></p>
<p>(b) Patch-based: AdvGlasses<br/></p>
<p>(c) GAN-based: AMT-GAN<br/></p>
<p>Figure 1: Illustration of different face encryption meth-<br/>ods [58, 20, 45]. The proposed DiffProtect produces nat-<br/>ural and imperceptible changes to the input images while<br/>achieving competitive attack success rates.<br/></p>
<p>based [3, 58] (Fig. 1a) and patch-based methods [27, 45]<br/>(Fig. 1b), which add unnatural and conspicuous changes<br/>to the source images. From a user-centered perspective,<br/>these approaches are not desirable in practice, as every-<br/>one wants to post their best-looking photos on social me-<br/>dia &#8211; not unattractive photos, even though they might be<br/>encrypted for protecting personal privacy.<br/></p>
<p>An ideal facial identity encryption algorithm should only<br/>create natural or imperceptible changes to the source im-<br/>ages. To achieve this goal, several works [59, 20, 38, 4]<br/>attempted to generate natural-looking adversarial examples<br/>using generative adversarial networks (GANs) [10, 25] by<br/>transferring make-up styles [59, 20] or editing facial at-<br/>tributes [38, 22]. Although these works have demonstrated<br/>promising results, the attack success rates (ASRs) of GAN-<br/>based methods can be lower than noise-based and patch-<br/>based methods since the output images are constrained<br/>on the learned manifold of a GAN. Moreover, most of<br/>the GAN-based facial encryption methods [20, 59] require<br/>training the model with a fixed victim identity. In other<br/>words, these models are target specific and we need to re-<br/>train the model when the target identity is changed. In addi-<br/></p>
<p>ar<br/>X<br/></p>
<p>iv<br/>:2<br/></p>
<p>30<br/>5.<br/></p>
<p>13<br/>62<br/></p>
<p>5v<br/>2 <br/></p>
<p> [<br/>cs<br/></p>
<p>.C<br/>V<br/></p>
<p>] <br/> 2<br/></p>
<p>8 <br/>M<br/></p>
<p>ay<br/> 2<br/></p>
<p>02<br/>3</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>tion, the visual quality of images generated by GAN-based<br/>methods is still dissatisfying (see Fig. 1c).<br/></p>
<p>Recently, diffusion models [18, 51, 50, 37, 24] have<br/>emerged as state-of-the-art generative models that are ca-<br/>pable of synthesizing realistic and high-resolution images.<br/>It has been shown that diffusion models perform better<br/>than GANs on many image generation tasks [6, 40, 37]<br/>and maintain better coverage of the image distribution [6].<br/>With the rise of diffusion models, a natural question ar-<br/>rives: can we utilize diffusion models as better generative<br/>models to generate adversarial examples with both high<br/>attack success rates and high visual quality? It has been<br/>shown that diffusion models can improve adversarial ro-<br/>bustness [12, 39, 54, 36, 1]. However, whether the strong<br/>generative capacity of diffusion models can be used to gen-<br/>erate adversarial attacks has not been explored.<br/></p>
<p>To this end, we propose DiffProtect, which utilizes a pre-<br/>trained diffusion autoencoder [37] to generate adversarial<br/>images for facial privacy protection. The overall pipeline<br/>of DiffProtect is shown in Fig. 2. We first encode an in-<br/>put face image I into a high-level semantic code z and a<br/>low-level noise code xT . We then iteratively optimize an<br/>adversarial semantic code zadv such that the resulting pro-<br/>tected image Ip generated by the conditional DDIM decod-<br/>ing process [37, 50] can fool the face recognition model. In<br/>this way, we can create semantically meaningful perturba-<br/>tion to the input image and utilize a diffusion model to gen-<br/>erate high-quality adversarial images (see Fig. 1d). We fur-<br/>ther introduce a face semantics regularization module to en-<br/>courage that Ip and I share similar face semantics to better<br/>preserve visual identity. Our extensive experiments on the<br/>CelebA-HQ [23] and FFHQ [25] datasets show that Diff-<br/>Protect produces protected face images of high visual qual-<br/>ity, while achieving significantly higher ASRs than the pre-<br/>vious best methods, e.g., + 24.5% absolute ASR on CelebA-<br/>HQ with IRSE50 [19] as the victim model.<br/></p>
<p>One drawback of diffusion models is the slow generation<br/>process, which requires many evaluation steps to generate<br/>one image. This can be problematic since we need to iter-<br/>atively optimize the semantic latent code. To address this<br/>issue, we propose an attack acceleration strategy that com-<br/>putes an approximated version of the reconstructed image<br/>at each attack iteration by running only one generative step,<br/>which drastically reduces the attack time while maintaining<br/>competitive attack performance.<br/></p>
<p>Our main contributions are summarized as follows:<br/>&#8226; We propose a novel diffusion model based attack<br/></p>
<p>method, termed DiffProtect, for facial privacy protec-<br/>tion, which crafts natural and inconspicuous adversar-<br/>ial examples on face recognition systems.<br/></p>
<p>&#8226; We further propose a face semantics regularization<br/>module to better preserve visual identity and a simple<br/></p>
<p>yet effective attack acceleration strategy to improve at-<br/>tack efficiency.<br/></p>
<p>&#8226; Our extensive experiments on the CelebA-HQ and<br/>FFHQ datasets demonstrate that DiffProtect produces<br/>more natural-looking encrypted images than state-of-<br/>the-art methods while achieving competitive attack<br/>performance.<br/></p>
<p>2. Related Work<br/>Adversarial Attacks with Generative Models While<br/>most of the existing adversarial attacks focus on optimiz-<br/>ing additive noises in the pixel space [11, 33, 2, 7], several<br/>works [55, 56, 21, 38, 59, 20] proposed to generate adver-<br/>sarial attacks with generative models, which produce per-<br/>ceptually realistic adversarial examples. Wong et al. [55]<br/>trained a conditional variational autoencoder (VAE) [49, 26]<br/>to generate a variety of perturbations and utilized the<br/>learned perturbation sets to improve model robustness and<br/>generalization. Xiao et al. [56] trained a conditional GAN<br/>to directly produce adversarial examples. [21, 31, 52] gen-<br/>erate on-manifold adversarial examples by optimizing la-<br/>tent codes in the latent space of GANs. Qiu et al. [38]<br/>generated semantically realistic adversarial examples by<br/>attribute-conditioned image editing using a GAN. To the<br/>best of our knowledge, this is the first work that utilizes<br/>diffusion models for generating adversarial attacks.<br/></p>
<p>Diffusion Models and Adversarial Robustness Diffu-<br/>sion models [48, 18, 35] are a family of generative models<br/>that model the target distribution by learning a reverse gen-<br/>erative denoising process. Recently, diffusion models have<br/>become state-of-the-art methods that can synthesize more<br/>realistic and high-resolution images with more stable train-<br/>ing process [18, 51, 50, 37, 24]. In the field of adversar-<br/>ial machine learning, it has been shown that diffusion mod-<br/>els can help to improve adversarial robustness and achieve<br/>state-of-the-art results [12, 39, 54, 36, 1]. [12, 39, 54] used<br/>diffusion models to generate synthetic data for adversarial<br/>training. [36, 1] exploited pretrained diffusion models as<br/>purification modules that remove the adversarial noise in<br/>the input images for empirical [36] and certified [1] de-<br/>fenses. However, it remains unknown whether the strong<br/>generative capacity of diffusion models can be used to gen-<br/>erate adversarial attacks and in this work we make the first<br/>attempt in this direction.<br/></p>
<p>Adversarial Attacks on Face Recognition Many studies<br/>have been proposed to attack FR systems, including both<br/>poisoning [44] and evasion [3, 20, 27, 45, 59, 58] attacks.<br/>Poisoning attacks require injecting poisoned face images<br/>into the training sets of FR systems, which is unlikely to<br/>achieve for individual users. Evasion attacks, especially</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>transferable black-box attacks, are more practical for pro-<br/>tecting facial image privacy, as they only require perturbing<br/>the source images to fool the FR systems at test time. Ex-<br/>isting attacks on FR systems [3, 20, 27, 45, 59, 58] suffer<br/>from poor visual quality or low attack success rates, which<br/>limit their usability in the real world. Noise-based meth-<br/>ods such as Lowkey [3] and TIP-IM [58] create unexplain-<br/>able noise patterns on face images. Patch-based methods<br/>such as Adv-Hat [27] and Adv-Glasses [45] add unnatural<br/>and conspicuous patches to the source images. GAN-based<br/>methods generate more natural-looking adversarial exam-<br/>ples [59, 20, 38, 22], but typically have lower attack success<br/>rates. In addition, [59, 20] drastically change the makeup<br/>styles of source images and might be biased towards female<br/>users [20]. In contrast, the proposed DiffProtect creates nat-<br/>ural and imperceptible changes to the source images and<br/>achieves competitive attack performance.<br/></p>
<p>3. Background: Diffusion Models<br/>Diffusion models [48, 18, 35] are a family of genera-<br/></p>
<p>tive models that model the target distribution by learning<br/>a reverse denoising generative process. A diffusion model<br/>consists of two processes: (1) a forward diffusion process<br/>that converts the input image x0 to noise map xT by gradu-<br/>ally adding noise in T forward steps; (2) a reverse denoising<br/>generative process that aims to recover x0 from xT by grad-<br/>ually denoising in T reverse steps.<br/></p>
<p>A Gaussian diffusion process gradually adds Gaussian<br/>noise to the data in the forward diffusion process:<br/></p>
<p>q(xt|xt&#8722;1) = N (<br/>&#8730;<br/></p>
<p>1&#8722; &#946;txt&#8722;1, &#946;tI), (1)<br/>where &#946;t (t = 1, ..., T ) are hyperparameters controlling the<br/>noise level at each diffusion step t. In the Gaussian diffu-<br/>sion process, the noisy image xt also follows a Gaussian<br/>distribution given x0:<br/></p>
<p>q(xt|x0) = N (<br/>&#8730;<br/>&#945;tx0, (1&#8722; &#945;t)I), (2)<br/></p>
<p>where &#945;t =<br/>&#8719;t<br/></p>
<p>s=1(1 &#8722; &#946;s). The goal of a diffusion model<br/>is to learn the reverse distribution p(xt&#8722;1|xt) so that we<br/>can generate data by sampling noise map xT from a prior<br/>distribution, e.g., N (0, I). When the difference between<br/>t&#8722;1 and t is infinitesimally small, i.e., T =&#8734;, p(xt&#8722;1|xt)<br/>can be modeled as [48, 18]:<br/></p>
<p>p(xt&#8722;1|xt) = N (&#181;&#952;(xt, t), &#963;<br/>2<br/>t I), (3)<br/></p>
<p>where &#181;&#952; is a neural network that predicts the posterior<br/>mean given the noisy image xt and time step t. In prac-<br/>tice, Ho et al. [18] proposed to train a U-Net [41] to learn a<br/>function &#1013;&#952;(xt, t) the noise that has been added to x0 by the<br/>following reweighted loss function:<br/></p>
<p>Lsimple =<br/>T&#8721;<br/></p>
<p>t=1<br/>Ex0,&#1013;t<br/></p>
<p>[<br/>&#8741;&#1013;t &#8722; &#1013;&#952;(xt, t)&#8741;2<br/></p>
<p>]<br/>, (4)<br/></p>
<p>where &#1013;t is the ground-truth noise added to x0 to produce<br/></p>
<p>xt. And &#181;&#952;(xt, t) can be obtained by:<br/>&#181;&#952;(xt, t) =<br/></p>
<p>1&#8730;<br/>1&#8722; &#946;t<br/></p>
<p>(<br/>xt &#8722;<br/></p>
<p>&#946;t&#8730;<br/>1&#8722; &#945;t<br/></p>
<p>&#1013;&#952;(xt, t)<br/>)<br/>. (5)<br/></p>
<p>Song et al. [50] proposed Denoising Diffusion Implicit<br/>Model (DDIM) that enjoys a deterministic generative pro-<br/>cess. DDIM has a non-Markovian forward process:<br/>q(xt&#8722;1|xt,x0) = N<br/></p>
<p>(&#8730;<br/>&#945;t&#8722;1x0 +<br/></p>
<p>&#8730;<br/>1&#8722; &#945;t&#8722;1<br/></p>
<p>xt&#8722;<br/>&#8730;<br/>&#945;tx0&#8730;<br/></p>
<p>1&#8722;&#945;t<br/>,0<br/></p>
<p>)<br/>.<br/></p>
<p>(6)<br/>In the reverse process, DDIM first predicts x0 given xt:<br/></p>
<p>f&#952;(xt, t) = (xt &#8722;<br/>&#8730;<br/>1&#8722; &#945;t &#183; &#1013;&#952;(xt, t))/<br/></p>
<p>&#8730;<br/>&#945;t, (7)<br/></p>
<p>and the reverse process is given by replacing x0 with<br/>f&#952;(xt, t) in Eq. (6):<br/>xt&#8722;1 =<br/></p>
<p>&#8730;<br/>&#945;t&#8722;1<br/></p>
<p>(<br/>xt&#8722;<br/></p>
<p>&#8730;<br/>1&#8722;&#945;t&#1013;&#952;(xt,t)&#8730;<br/></p>
<p>&#945;t<br/></p>
<p>)<br/>+<br/>&#8730;<br/>1&#8722; &#945;t&#8722;1&#1013;&#952;(xt, t).<br/></p>
<p>(8)<br/>We can think of DDIM as an image encoder, where we<br/>can run the generative process backward deterministically<br/>to obtain a noise map xT that serves as a latent variable<br/>representing x0, as well as an image decoder, where we run<br/>the generative process (Eq. (8)) to decoder x0 from xT .<br/></p>
<p>4. DiffProtect<br/>4.1. Problem Formulation<br/></p>
<p>In this section, we formulate the problem of adversarial<br/>attacks on face recognition systems. Suppose the face im-<br/>ages I &#8712; I := RH&#215;W&#215;C are drawn from an underlying<br/>distribution PI , where H , W and C are the height, width<br/>and the number of channels of the image respectively. Let<br/>h(I) : I &#8594; Rd denote a face recognition model which<br/>maps an input face image in I to a feature vector in Rd. An<br/>accurate FR system can map two images I1 and I2 of the<br/>same identity to features that are close in the feature space,<br/>and to features that are far away when they are of differ-<br/>ent identities, i.e., D(h(I1), h(I2)) &#8804; &#964; when y1 = y2 and<br/>D(h(I1), h(I2)) &gt; &#964; when y1 &#824;= y2, where D is a distance<br/>function, &#964; is a threshold and y1, y2 are the identity of I1<br/>and I2 respectively.<br/></p>
<p>In the untargeted attack or &#8220;dodging attack&#8221; setting, a<br/>successful attack fools the face recognition system to map<br/>an adversarial image Iadv with the same identity as I to a fea-<br/>ture that is far away from h(I), i.e., D(h(I), h(Iadv)) &gt; &#964; .<br/>In the targeted attack or &#8220;impersonation attack&#8221; setting, a<br/>successful attack fools the face recognition system to map<br/>an adversarial image Iadv with the targeted identity to a fea-<br/>ture that is close to h(I), i.e. D(h(I), h(Iadv)) &#8804; &#964; . In this<br/>work, we focus on targeted attack setting following previ-<br/>ous work [61, 38, 20].<br/>4.2. Detailed Construction<br/></p>
<p>The overall pipeline of DiffProtect is shown in Fig. 2.<br/>DiffProtect consists of a semantic encoder and a conditional<br/>DDIM that serves as a stochastic encoder and an image de-</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Semantic&#160;<br/>Encoder<br/></p>
<p>input image<br/></p>
<p>segmentation map&#160;&#160;<br/></p>
<p>Update<br/>semantic code Face Recognition<br/></p>
<p>Conditional DDIM Decoding<br/></p>
<p>protected image<br/>Face Semantics<br/>Regularization<br/></p>
<p>target identity<br/></p>
<p>Conditional DDIM<br/>Encoding<br/></p>
<p>stochastic code<br/></p>
<p>Figure 2: Overview of DiffProtect. DiffProtect utilizes a diffusion autoencoder [37], which encodes the input image as a<br/>semantic code z and a stochastic code xT . We iteratively optimize an adversarial semantic code zadv such that the resulting<br/>protected image Ip generated by the conditional DDIM decoding process can fool the face recognition model.<br/></p>
<p>coder. An input face image is first encoded as a high-level<br/>semantic code z and a stochastic code xT that captures low-<br/>level variations. We aim to optimize an adversarial semantic<br/>code zadv such that the resulting protected image Ip gen-<br/>erated by the conditional DDIM decoding process [37, 50]<br/>can fool the face recognition model to protect facial privacy.<br/></p>
<p>Semantic Encoder The semantic encoder learns to map<br/>an input face image I into a semantic latent code z =<br/>Enc(I) that captures high-level face semantics. Manipu-<br/>lating z results in semantic changes in the image.<br/></p>
<p>Conditional DDIM The conditional DDIM [37] is a<br/>DDIM model conditioned on the semantic code z, where<br/>we train a noise prediction network &#1013;&#952;(xt, t, z) with z as an<br/>additional input. During the decoding process, we obtain<br/>the reconstructed image I = x0 = DDIMdec(xT , z), by<br/>running the following deterministic generative process:<br/></p>
<p>p&#952;(xt&#8722;1|xt, z) =<br/>{<br/>N (f&#952;(x1, 1, z),0) if t = 1<br/>q(xt&#8722;1|xt, f&#952;(xt, t, z)) otherwise,<br/></p>
<p>(9)<br/>where f&#952;(xt, t, z) = (xt&#8722;<br/></p>
<p>&#8730;<br/>1&#8722; &#945;t &#183;&#1013;&#952;(xt, t, z))/<br/></p>
<p>&#8730;<br/>&#945;t, and<br/></p>
<p>q(&#183;|&#183;, &#183;) is defined in Eq. (6).<br/>During the encoding process, we obtain the stochastic<br/></p>
<p>code of the input image xT = DDIMenc(I, z) by running<br/>the deterministic generative process backward:<br/>xt+1 =<br/></p>
<p>&#8730;<br/>&#945;t+1f&#952;(xt, t, z)+<br/></p>
<p>&#8730;<br/>1&#8722; &#945;t+1&#1013;&#952;(xt, t, z). (10)<br/></p>
<p>xT is encouraged to encode only the information left out by<br/>z, i.e., the stochastic details.<br/></p>
<p>Attack Formulation To generate a protected image Ip<br/>that can effectively fool the face recognition systems and<br/>has a high visual quality to human eyes, we perturb the se-<br/>mantic code z of the input image I to obtain an adversarial<br/></p>
<p>semantic code zadv to create semantically meaningful per-<br/>turbations. We then generate Ip by feeding zadv and xT to<br/>the DDIM decoding process:<br/></p>
<p>Ip = DDIMdec(xT , zadv). (11)<br/>In this way, we constrain Ip to lie on the manifold of real<br/>image distribution to ensure that Ip has high visual quality.<br/></p>
<p>Formally, for targeted privacy protection, we aim to<br/>solve the following optimization problem:<br/>min<br/>zadv<br/>Liden(Ip) = D(h(Ip), h(It)), s.t. &#8741;zadv &#8722; z&#8741;&#8734; &lt; &#947;,<br/></p>
<p>(12)<br/>where It is the face image of the target identity, Ip is the<br/>protected face image generated as Eq. (11), &#947; is the attack<br/>budget, D is the cosine distance, h is the face recognition<br/>model. In practice, the user may not have access to h. In<br/>such a black-box attack setting, we use surrogate models<br/>g &#8712; Ag to estimate the identity loss Liden:<br/></p>
<p>Liden(Ip) =<br/>&#8721;<br/>g&#8712;Ag<br/></p>
<p>D(g(Ip), g(It)). (13)<br/></p>
<p>Face Semantics Regularization Although the attack<br/>strength is controlled by a small attack budget &#947; in Eq. (12)<br/>to preserve the overall facial identity to human eyes, in some<br/>cases the protected image can have different local facial fea-<br/>tures compared to the input image in order to match the<br/>characteristics of the target image, such as the face shape in<br/>Fig. 7, which may not be desired by some users.<br/></p>
<p>To regularize the face semantics of the protected image,<br/>we propose a face semantic consistency lossLsem that com-<br/>putes the similarity between the semantic maps of the pro-<br/>tected image Ip and the input image I:<br/></p>
<p>Lsem(Ip) = CE(M(Ip),M(I)), (14)<br/>whereM is a face parsing network that outputs the seman-<br/>tic map of the input image, and CE is the cross-entropy loss.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Algorithm 1 DiffProtect<br/>1: Input: I, N , &#947;, &#951;, &#955;, T , Enc, &#1013;&#952;, h or Ag ,M<br/>2: Output: protected image Ip<br/>3: &#9655; Image Encoding<br/>4: z = Enc(I), xT = DDIMenc(I, z)<br/>5: &#9655; Attack Generation<br/>6: z<br/></p>
<p>(0)<br/>adv &#8592; z<br/></p>
<p>7: for i = 0 to N &#8722; 1<br/>8: I<br/></p>
<p>(i)<br/>p = DDIMdec(xT , z<br/></p>
<p>(i)<br/>adv)<br/></p>
<p>9: or I<br/>(i)<br/>p &#8776; f&#952;(xt0 , t0, z<br/></p>
<p>(i)<br/>adv) &#9655; DiffProtect-fast<br/></p>
<p>10: L(I(i)p ) = Liden(I<br/>(i)<br/>p ) + &#955; &#183; Lsem(I<br/></p>
<p>(i)<br/>p )<br/></p>
<p>11: z<br/>(i+1)<br/>adv = PS<br/></p>
<p>(<br/>z<br/>(i)<br/>adv &#8722; &#951; &#183; sign<br/></p>
<p>(<br/>&#8711;<br/></p>
<p>z<br/>(i)<br/>adv<br/>L(I(i)p )<br/></p>
<p>))<br/>12: end for<br/>13: zadv &#8592; z<br/></p>
<p>(N)<br/>adv , Ip = DDIMdec(xT , zadv)<br/></p>
<p>The optimization problem in Eq. (12) becomes:<br/>min<br/>zadv<br/>L(Ip) = Liden(Ip)+&#955;&#183;Lsem(Ip), s.t. &#8741;zadv &#8722; z&#8741;&#8734; &lt; &#947;,<br/></p>
<p>(15)<br/>where &#955; is a hyper-parameter that controls the importance<br/>of the semantic consistency term.<br/>Attack Generation The DiffProtect algorithm is summa-<br/>rized in Alg. 1. We iteratively solve Eq. (15) with projected<br/>gradient descent [33]:&#63729;&#63730;&#63731; I(i)p = DDIMdec(xT , z<br/></p>
<p>(i)<br/>adv) (16)<br/></p>
<p>z<br/>(i+1)<br/>adv = PS<br/></p>
<p>(<br/>z<br/>(i)<br/>adv &#8722; &#951; &#183; sign<br/></p>
<p>(<br/>&#8711;<br/></p>
<p>z<br/>(i)<br/>adv<br/>L(I(i)p )<br/></p>
<p>))<br/>(17)<br/></p>
<p>where z(0)adv = z, &#951; is the attack step size,PS is the projection<br/>onto feasible set S = {zadv : &#8741;zadv &#8722; z&#8741;&#8734; &lt; &#947;}. Note that<br/>the DDIM decoding process is fully differentiable (Eq. (9))<br/>so we can directly compute&#8711;zadvL(Ip) to update zadv.<br/></p>
<p>Attack Acceleration In Eq. (16), we need to run the full<br/>DDIM decoding process to obtain I<br/></p>
<p>(i)<br/>p at each attack itera-<br/></p>
<p>tion. The time complexity for generating a protected image<br/>is O(T &#215; N), where T is the number of generative steps<br/>in the DDIM decoding process, N is the number of attack<br/>steps. To accelerate attack generation, we can instead com-<br/>pute an approximated version of I(i)p at each attack iteration<br/>by running only one generative step. Specifically, let t0 be<br/>the time stamp from which we run the generative step to es-<br/>timate I(i)p . We can obtain the corresponding noise map xt0<br/>from the DDIM encoding process and estimate I<br/></p>
<p>(i)<br/>p by:<br/></p>
<p>I(i)p &#8776; f&#952;(xt0 , t0, z<br/>(i)<br/>adv) =<br/></p>
<p>1<br/>&#8730;<br/>&#945;t0<br/></p>
<p>(xt0&#8722;<br/>&#8730;<br/></p>
<p>1&#8722; &#945;t0 &#183;&#1013;&#952;(xt0 , t0, z<br/>(i)<br/>adv)).<br/></p>
<p>(18)<br/>In this way, we can reduce the time complexity to<br/>O(N). We denote this accelerated version of DiffProtect<br/>as DiffProtect-fast. Note that xt0 is not updated during the<br/></p>
<p>attack. This simple strategy works surprisingly well as we<br/>will show in Sec. 5.2. The effect of the choice of t0 is shown<br/>in the supplementary materials.<br/></p>
<p>5. Experiments<br/>5.1. Experimental Settings<br/>Dataset We evaluate DiffProtect on two commonly used<br/>high-quality face image datasets: CelebA-HQ [23] and<br/>FFHQ [25]. Following [20], we use a subset of 1000 face<br/>images with different identities for each dataset.<br/></p>
<p>Attack Setting In this paper, we focus on black-box tar-<br/>geted attacks following previous work [61, 38, 20]. We<br/>consider three popular face recognition models, including<br/>IR152 [14], IRSE50 [19], and MobileFace [5] as the victim<br/>models. For each model, we use the other models as the<br/>surrogate models Ag to craft black-box attacks.<br/></p>
<p>Evaluation Metrics We use Attack Success Rate<br/>(ASR) [20, 61] to evaluate the attack performance:<br/></p>
<p>ASR =<br/>1<br/>K<br/></p>
<p>&#8721;<br/>I<br/></p>
<p>I (cos(h(It), h(Ip)) &gt; &#964;)&#215; 100%, (19)<br/></p>
<p>where I is the indicator function, K is the number of face<br/>image I, &#964; is the threshold, It and Ip are the target and pro-<br/>tected face images respectively. The value of &#964; is set at 0.01<br/>False Acceptance Rate (FAR) for each victim model. In<br/>addition, we use Frechet Inception Distance (FID) [16] to<br/>evaluate the naturalness of protected face images.<br/></p>
<p>Implementation Details The architecture of DiffProtect<br/>is mainly based on Diffusion Autoencoder (DiffAE) [37].<br/>We use the DiffAE model trained on the FFHQ dataset<br/>with 256 &#215; 256 image resolutions. The DiffAE model is<br/>fixed during attack generation. For face semantics regu-<br/>larization, we use a pretrained BiSeNet [60] trained on the<br/>CelebAMask-HQ dataset [28] as the face parsing network<br/>M. We set N = 50, &#947; = 0.03, &#951; = 2 &#183; &#947;<br/></p>
<p>N and &#955; = 0 by<br/>default. We set T = 100 for conditional DDIM encoding<br/>and final decoding, but use five decoding steps for recon-<br/>structing I<br/></p>
<p>(i)<br/>p during attack (Eq. (16)) in order to save time<br/></p>
<p>and memory. For DiffProtect-fast, we set to = 60. All ex-<br/>periments were run on a server with 8 Nvidia A5000 GPUs.<br/>5.2. Comparison with the state-of-the-art<br/>Main Results We compare DiffProtect with state-of-the-<br/>art face encryption methods, including PGD [33], MIM [7],<br/>TIP-IM [58], and AMT-GAN [20]. The implementation de-<br/>tails of baseline methods can be found in the supplemen-<br/>tary materials. Table 1 reports the quantitative results on<br/>the CelebA-HQ and FFHQ datasets. For ASR, DiffPro-<br/>tect significantly outperforms previous methods by a large</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Methods Target-<br/>specific<br/></p>
<p>Natural-<br/>looking<br/></p>
<p>CelebA-HQ FFHQ<br/>ASR (%) &#8593; FID &#8595; ASR (%) &#8593; FID &#8595;IRSE50 IR152 MobileFace IRSE50 IR152 MobileFace<br/></p>
<p>No attack &#10007; &#10003; 7.3 3.8 1.1 0 4.4 2.5 5.2 0<br/>PGD [33] &#10007; &#10007; 32.6 19.0 35.6 39.6 20.1 14.4 18.8 38.1<br/>MIM [7] &#10007; &#10007; 37.4 31.0 35.5 69.9 24.5 23.2 21.4 62.1<br/>TIP-IM [58] &#10007; &#10007; 47.2 35.3 45.3 71.6 31.0 27.5 26.9 62.8<br/>AMT-GAN [20] &#10003; &#10003; 53.9 41.9 60.0 31.1 32.6 30.5 29.9 30.5<br/>DiffProtect-fast (&#955; = 0) &#10007; &#10003; 68.4 49.8 72.1 26.7 50.8 47.6 47.0 26.4<br/>DiffProtect (&#955; = 0) &#10007; &#10003; 78.4 60.3 77.9 27.6 57.7 54.3 52.9 26.1<br/>DiffProtect (&#955; = 0.2) &#10007; &#10003; 67.7 48.7 69.3 24.4 46.2 45.4 44.3 23.5<br/></p>
<p>Table 1: Comparison with state-of-the-art methods on CelebA-HQ and FFHQ datasets for targeted black-box attacks. The<br/>best performance is in bold and the second best is underlined.<br/></p>
<p>In<br/>pu<br/></p>
<p>t<br/>T<br/></p>
<p>IP<br/>-I<br/></p>
<p>M<br/>[5<br/></p>
<p>8]<br/>A<br/></p>
<p>M<br/>T-<br/></p>
<p>G<br/>A<br/></p>
<p>N<br/>[2<br/></p>
<p>0]<br/>D<br/></p>
<p>iff<br/>Pr<br/></p>
<p>ot<br/>ec<br/></p>
<p>t-<br/>fa<br/></p>
<p>st<br/>D<br/></p>
<p>iff<br/>Pr<br/></p>
<p>ot<br/>ec<br/></p>
<p>t<br/></p>
<p>Figure 3: Visualizations of the protected face images generated by different face encryption methods on CelebA-HQ.<br/></p>
<p>margin. In addition, DiffProtect achieves the lowest FID<br/>scores, which indicates that the encrypted images generated<br/>by DiffProtect are the most natural. We show some exam-<br/>ples of protected face images in Fig. 3 with &#955; = 0. We can<br/>observe that DiffProtect produces good-looking protected<br/>images with natural and inconspicuous changes to the in-<br/>put images, such as slight changes in facial expressions. It<br/></p>
<p>works well across genders, ages, and races, and in some<br/>cases even makes the images look more attractive. Com-<br/>pared to TIP-IM, the protected face images generated by<br/>DiffProtect have no obvious noise pattern as we only per-<br/>turb the semantic codes and generate the images through<br/>a conditional DDIM. Compared to AMT-GAN, DiffPro-<br/>tect can better preserve image styles and details and does</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>IR<br/>SE<br/></p>
<p>50<br/></p>
<p>None Feature<br/>Squeezing<br/></p>
<p>JPEG<br/>Compression<br/></p>
<p>Median<br/>Blur<br/></p>
<p>Diffusion<br/>Purification<br/></p>
<p>Defense Method<br/></p>
<p>20<br/>30<br/>40<br/>50<br/>60<br/>70<br/>80<br/>90<br/></p>
<p>AS<br/>R <br/></p>
<p>(%<br/>)<br/></p>
<p>PGD<br/>MIM<br/></p>
<p>AMT-GAN<br/>TIP-IM<br/></p>
<p>DiffProtect<br/></p>
<p>None Feature<br/>Squeezing<br/></p>
<p>JPEG<br/>Compression<br/></p>
<p>Median<br/>Blur<br/></p>
<p>Diffusion<br/>Purification<br/></p>
<p>Defense Method<br/></p>
<p>10<br/></p>
<p>20<br/></p>
<p>30<br/></p>
<p>40<br/></p>
<p>50<br/></p>
<p>60<br/></p>
<p>70<br/></p>
<p>AS<br/>R <br/></p>
<p>(%<br/>)<br/></p>
<p>PGD<br/>MIM<br/></p>
<p>AMT-GAN<br/>TIP-IM<br/></p>
<p>DiffProtect(Ours)<br/></p>
<p>IR<br/>15<br/></p>
<p>2<br/></p>
<p>None Feature<br/>Squeezing<br/></p>
<p>JPEG<br/>Compression<br/></p>
<p>Median<br/>Blur<br/></p>
<p>Diffusion<br/>Purification<br/></p>
<p>Defense Method<br/></p>
<p>10<br/></p>
<p>20<br/></p>
<p>30<br/></p>
<p>40<br/></p>
<p>50<br/></p>
<p>60<br/></p>
<p>70<br/></p>
<p>AS<br/>R <br/></p>
<p>(%<br/>)<br/></p>
<p>PGD<br/>MIM<br/></p>
<p>AMT-GAN<br/>TIP-IM<br/></p>
<p>DiffProtect<br/></p>
<p>None Feature<br/>Squeezing<br/></p>
<p>JPEG<br/>Compression<br/></p>
<p>Median<br/>Blur<br/></p>
<p>Diffusion<br/>Purification<br/></p>
<p>Defense Method<br/></p>
<p>10<br/></p>
<p>20<br/></p>
<p>30<br/></p>
<p>40<br/></p>
<p>50<br/></p>
<p>60<br/></p>
<p>AS<br/>R <br/></p>
<p>(%<br/>)<br/></p>
<p>PGD<br/>MIM<br/></p>
<p>AMT-GAN<br/>TIP-IM<br/></p>
<p>DiffProtect<br/></p>
<p>M<br/>ob<br/></p>
<p>ile<br/>Fa<br/></p>
<p>ce<br/></p>
<p>None Feature<br/>Squeezing<br/></p>
<p>JPEG<br/>Compression<br/></p>
<p>Median<br/>Blur<br/></p>
<p>Diffusion<br/>Purification<br/></p>
<p>Defense Method<br/></p>
<p>20<br/></p>
<p>30<br/></p>
<p>40<br/></p>
<p>50<br/></p>
<p>60<br/></p>
<p>70<br/></p>
<p>80<br/></p>
<p>90<br/></p>
<p>AS<br/>R <br/></p>
<p>(%<br/>)<br/></p>
<p>PGD<br/>MIM<br/></p>
<p>AMT-GAN<br/>TIP-IM<br/></p>
<p>DiffProtect<br/></p>
<p>(a) CelebA-HQ<br/></p>
<p>None Feature<br/>Squeezing<br/></p>
<p>JPEG<br/>Compression<br/></p>
<p>Median<br/>Blur<br/></p>
<p>Diffusion<br/>Purification<br/></p>
<p>Defense Method<br/></p>
<p>10<br/></p>
<p>20<br/></p>
<p>30<br/></p>
<p>40<br/></p>
<p>50<br/></p>
<p>60<br/></p>
<p>AS<br/>R <br/></p>
<p>(%<br/>)<br/></p>
<p>PGD<br/>MIM<br/></p>
<p>AMT-GAN<br/>TIP-IM<br/></p>
<p>DiffProtect<br/></p>
<p>(b) FFHQ<br/>Figure 4: ASR under various defense methods.<br/></p>
<p>not require training a target-specific model for each iden-<br/>tity. Our accelerated method DiffProtect-fast also achieves<br/>higher ASRs and lower FID compared to the baselines. In<br/>addition, we can observe that DiffProtect and DiffProtect-<br/>fast achieve similar visual quality, while DiffProtect-fast re-<br/>quires 50% less computation time (more details in the sup-<br/>plementary material).<br/></p>
<p>ASR under Defenses We further evaluate the effective-<br/>ness of DiffProtect against four adversarial defense meth-<br/>ods, including feature squeezing [57], median blurring [29],<br/>JPEG compression [8], and a state-of-the-art diffusion-<br/>based defense DiffPure [36]. The implementation details<br/>of the defenses can be found in the supplementary materi-<br/>als. From Fig. 4 we can see that DiffProtect still achieves<br/>higher ASR under various defenses. In addition, DiffProtect<br/>is much more resilient to the DiffPure defense compared to<br/>baseline methods since it generates semantically meaning-<br/>ful perturbations to the input images that are hard to remove<br/>during the diffusion denoising process of DiffPure.<br/>5.3. Ablation Studies<br/></p>
<p>For ablation studies, we use the CelebA dataset and Mo-<br/>bileFace as the victim model. We set N = 10, &#947; = 0.02,<br/>and T = 50. The other settings are the same as Sec. 5.1.<br/></p>
<p>GAN vs. Diffusion model To have a fair compari-<br/>son between GAN-based and Diffusion-based methods,<br/></p>
<p>Dataset Model IRSE50 IR152 MobileFace<br/>CelebA-HQ GAN 33.0 16.5 41.1<br/></p>
<p>Diff. 60.7 (+27.7) 37.5 (+21.0) 62.0 (+20.6)<br/>FFHQ GAN 21.3 20.0 21.2<br/></p>
<p>Diff. 36.5 (+15.2) 33.1 (+13.1) 34.0 (+12.8)<br/>Table 2: ASR (%) of GAN [53] and Diffusion [37] model.<br/></p>
<p>(a) Input image (b) GAN output. (c) Diffusion output.<br/>Figure 5: Visualizations of adversarial images generated by<br/>GAN [53] and diffusion [37] models on CelebA-HQ.<br/></p>
<p>we re-implement our method using a GAN-based autoen-<br/>coder [53] where we also iteratively optimize the GAN la-<br/>tent code using the same formulation. The implementation<br/>details can be found in the supplementary materials. The<br/>results are summarized in Table 2. We can observe that<br/>the diffusion-based method has much higher ASRs than the<br/>GAN-based method. From Fig. 5 we can observe that the<br/>image quality of Diffusion model output is also visually<br/>better than the GAN-based method. These results confirm<br/>our intuition that a better generative model like a Diffusion<br/>model can help to improve both attack performance and im-<br/>age quality.<br/></p>
<p>Effect of Attack Budget We show the effect of attack<br/>budget &#947; in Fig. 6. We can observe that as &#947; increases, the<br/>ASR also increases; when &#947; = 0, we have ASR = 97.7%.<br/>However, the FID also goes up as &#947; increases, which indi-<br/>cates that the image quality becomes worse. In addition, a<br/>large &#947; can cause the protected image to lose the visual iden-<br/>tity of the input image. In practice, we recommend setting<br/>&#947; &#8804; 0.03 such that the output images have high visual qual-<br/>ity and preserve the identity of the input images, which is<br/>confirmed by a user study using Amazon Mechanical Turk<br/>(see the supplementary material).<br/></p>
<p>Effect of Face Semantics Regularization We show the<br/>effect of face semantics regularization in Fig. 7. Without the<br/>face semantics regularization term (&#955; = 0), the protected</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Input Image &#947; = 0.005 &#947; = 0.01 &#947; = 0.02 &#947; = 0.03 &#947; = 0.05 &#947; = 0.1 Target Image<br/></p>
<p>&#947; increasing&#8722;&#8722;&#8722;&#8722;&#8722;&#8722;&#8722;&#8722;&#8722;&#8594; 25.0%, 28.5 37.4%, 30.23 60.5%, 33.42 75.4%, 36.42 88.4%, 42.53 97.7%, 57.25<br/>Figure 6: Visual comparisons of different attack budget &#947; and the corresponding ASR and FID.<br/></p>
<p>Target identity&#160;<br/></p>
<p>Input Image<br/></p>
<p>(a)<br/></p>
<p>(c)<br/></p>
<p>(b)<br/></p>
<p>(d)<br/>Figure 7: Effect of Face Semantics Regularization. (a)-(d):<br/>protected image Ip generated with different &#955; in Eq. (15).<br/>Increasing &#955; gradually changes the face shape from a heart<br/>shape (similar to It) to a square shape (similar to I).<br/></p>
<p>&#955; 0 0.01 0.05 0.1 0.2 0.5<br/>ASR (%) &#8593; 60.5 60.3 58.9 56.3 53.5 46.1<br/></p>
<p>FID &#8595; 33.4 33.3 32.8 32.4 31.9 31.2<br/>Table 3: Quantitative results of face semantics regulariza-<br/>tion with different &#955; in Eq. (15).<br/></p>
<p>image can have different local facial features compared to<br/>the input image to match the characteristics of the target im-<br/>age, such as the face shape in Fig. 7 (a). Increasing the value<br/>of &#955; helps to preserve the face features of the input images.<br/>Quantitatively, from Table 3 we can observe that increasing<br/>&#955; helps to reduce FID, which indicates better image qual-<br/>ity but also decreases ASR. In practice, a user can choose a<br/>value of &#955; as well as the attack budget &#947; depending on their<br/>own preference.<br/></p>
<p>Method<br/></p>
<p>C<br/>on<br/></p>
<p>fid<br/>en<br/></p>
<p>ce<br/></p>
<p>0<br/></p>
<p>10<br/></p>
<p>20<br/></p>
<p>30<br/></p>
<p>40<br/></p>
<p>50<br/></p>
<p>60<br/></p>
<p>70<br/></p>
<p>Clean PGD MIM AMT-GAN TIP-IM DiffProtect (Ours)<br/></p>
<p>CelebA-HQ FFHQ<br/><b>Confidence score evaluated by Face++<br/></b></p>
<p>Figure 8: Confidence scores of different methods evaluated<br/>by commercial API Face++.<br/></p>
<p>5.4. Evaluation on Commercial API<br/>We further evaluate the effectiveness of AGE-FTM in the<br/></p>
<p>real world using a commercial FR API Face++1, where we<br/>compute the confidence scores between the protected im-<br/>ages and the target identity image. DiffProtect achieves av-<br/>erage confidence scores of 62.8% and 58.03% for CelebA-<br/>HQ and FFHQ respectively, which are higher than baseline<br/>methods. This shows that the proposed method has a sat-<br/>isfactory attack ability even for the commercial API while<br/>achieving good image quality.<br/></p>
<p>6. Conclusion<br/>In this work, we propose DiffProtect a novel diffusion-<br/></p>
<p>based method for facial privacy protection. It generates<br/>semantically meaningful perturbations to the input images<br/>and produces adversarial images of high visual quality. Our<br/>extensive experiments on CelebA-HQ and FFHQ demon-<br/>strate that DiffProtect significantly outperforms previous<br/>state-of-the-art methods for both attack performance and<br/>image quality.<br/></p>
<p>1https://www.faceplusplus.com/</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>References<br/>[1] Nicholas Carlini, J Zico Kolter, Florian Tramer, Krishna-<br/></p>
<p>murthy Dj Dvijotham, Leslie Rice, and Mingjie Sun. (certi-<br/>fied!!) adversarial robustness for free! In ICLR, 2023. 2<br/></p>
<p>[2] Nicholas Carlini and David Wagner. Towards evaluating the<br/>robustness of neural networks. In IEEE Symposium on Secu-<br/>rity and Srivacy (SP), 2017. 2<br/></p>
<p>[3] Valeriia Cherepanova, Micah Goldblum, Harrison Foley,<br/>Shiyuan Duan, John Dickerson, Gavin Taylor, and Tom<br/>Goldstein. Lowkey: Leveraging adversarial attacks to pro-<br/>tect social media users from facial recognition. In ICLR,<br/>2021. 1, 2, 3<br/></p>
<p>[4] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha,<br/>Sunghun Kim, and Jaegul Choo. StarGAN: Unified Gen-<br/>erative Adversarial Networks for Multi-Domain Image-to-<br/>Image Translation. In CVPR, 2018. 1<br/></p>
<p>[5] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos<br/>Zafeiriou. Arcface: Additive angular margin loss for deep<br/>face recognition. In CVPR, 2019. 1, 5<br/></p>
<p>[6] Prafulla Dhariwal and Alexander Nichol. Diffusion models<br/>beat gans on image synthesis. In NeurIPS, 2021. 2<br/></p>
<p>[7] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun<br/>Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial at-<br/>tacks with momentum. In CVPR, 2018. 2, 5, 6, 11<br/></p>
<p>[8] Gintare Karolina Dziugaite, Zoubin Ghahramani, and<br/>Daniel M Roy. A study of the effect of jpg compression on<br/>adversarial images. arXiv preprint arXiv:1608.00853, 2016.<br/>7<br/></p>
<p>[9] Gintare Karolina Dziugaite, Zoubin Ghahramani, and<br/>Daniel M Roy. A study of the effect of jpg compression on<br/>adversarial images. arXiv preprint arXiv:1608.00853, 2016.<br/>11<br/></p>
<p>[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing<br/>Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and<br/>Yoshua Bengio. Generative adversarial nets. In NeurIPS,<br/>2014. 1<br/></p>
<p>[11] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.<br/>Explaining and harnessing adversarial examples. In ICLR,<br/>2015. 2<br/></p>
<p>[12] Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Flo-<br/>rian Stimberg, Dan A. Calian, and Timothy Mann. Im-<br/>proving robustness using generated data. arXiv preprint<br/>arXiv:2110.09468, 2021. 2<br/></p>
<p>[13] Shuangchi Gu, Ping Yi, Ting Zhu, Yao Yao, and Wei Wang.<br/>Detecting adversarial examples in deep neural networks us-<br/>ing normalizing filters. UMBC Student Collection, 2019. 11<br/></p>
<p>[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.<br/>Deep residual learning for image recognition. In Proceed-<br/>ings of the IEEE conference on computer vision and pattern<br/>recognition, pages 770&#8211;778, 2016. 5<br/></p>
<p>[15] Yonghao He, Dezhong Xu, Lifang Wu, Meng Jian, Shiming<br/>Xiang, and Chunhong Pan. LFFD: A light and fast face de-<br/>tector for edge devices. arXiv preprint arXiv:1904.10633,<br/>2019. 1<br/></p>
<p>[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,<br/>Bernhard Nessler, and Sepp Hochreiter. Gans trained by a<br/></p>
<p>two time-scale update rule converge to a local nash equilib-<br/>rium. Advances in neural information processing systems,<br/>30, 2017. 5<br/></p>
<p>[17] Kashmir Hill. The secretive company that might end privacy<br/>as we know it. In Ethics of Data and Analytics, pages 170&#8211;<br/>177. Auerbach Publications, 2020. 1<br/></p>
<p>[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-<br/>sion probabilistic models. NeurIPS, 2020. 2, 3<br/></p>
<p>[19] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-<br/>works. In CVPR, 2018. 2, 5<br/></p>
<p>[20] Shengshan Hu, Xiaogeng Liu, Yechao Zhang, Minghui Li,<br/>Leo Yu Zhang, Hai Jin, and Libing Wu. Protecting facial pri-<br/>vacy: Generating adversarial identity masks via style-robust<br/>makeup transfer. In CVPR, 2022. 1, 2, 3, 5, 6, 11, 13, 14<br/></p>
<p>[21] Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, and<br/>Alexandros G Dimakis. The robust manifold defense: Ad-<br/>versarial training using generative models. arXiv preprint<br/>arXiv:1712.09196, 2017. 2<br/></p>
<p>[22] Shuai Jia, Bangjie Yin, Taiping Yao, Shouhong Ding, Chun-<br/>hua Shen, Xiaokang Yang, and Chao Ma. Adv-Attribute:<br/>Inconspicuous and Transferable Adversarial Attack on Face<br/>Recognition. In NeurIPS, 2022. 1, 3<br/></p>
<p>[23] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.<br/>Progressive growing of GANs for improved quality, stability,<br/>and variation. In ICLR, 2018. 2, 5<br/></p>
<p>[24] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.<br/>Elucidating the design space of diffusion-based generative<br/>models. In NeurIPS, 2022. 2<br/></p>
<p>[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based<br/>generator architecture for generative adversarial networks. In<br/>CVPR, 2019. 1, 2, 5<br/></p>
<p>[26] Diederik P. Kingma and Max Welling. Auto-encoding vari-<br/>ational bayes. In ICLR, 2014. 2<br/></p>
<p>[27] Stepan Komkov and Aleksandr Petiushko. AdvHat: Real-<br/>world adversarial attack on ArcFace face ID system. In<br/>ICPR, 2021. 1, 2, 3<br/></p>
<p>[28] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo.<br/>MaskGAN: Towards Diverse and Interactive Facial Image<br/>Manipulation. In CVPR, 2020. 5<br/></p>
<p>[29] Xin Li and Fuxin Li. Adversarial examples detection in deep<br/>networks with convolutional filter statistics. In ICCV, 2017.<br/>7<br/></p>
<p>[30] Xin Li and Fuxin Li. Adversarial examples detection in deep<br/>networks with convolutional filter statistics. In Proceedings<br/>of the IEEE international conference on computer vision,<br/>pages 5764&#8211;5772, 2017. 11<br/></p>
<p>[31] Wei-An Lin, Chun Pong Lau, Alexander Levine, Rama Chel-<br/>lappa, and Soheil Feizi. Dual Manifold Adversarial Robust-<br/>ness: Defense against Lp and non-Lp Adversarial Attacks.<br/>In NeurIPS, 2020. 2<br/></p>
<p>[32] Taylor Kay Lively. Facial recognition in the US: Privacy con-<br/>cerns and legal developments. rb.gy/u8i6ny. Accessed:<br/>2/14/2023. 1<br/></p>
<p>[33] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,<br/>Dimitris Tsipras, and Adrian Vladu. Towards deep learning<br/>models resistant to adversarial attacks. In ICLR, 2018. 2, 5,<br/>6, 11</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>[34] Paul Mozur and Aaron Krolik. A surveillance net blankets<br/>China&#8217;s cities, giving police vast powers. The New York<br/>Times, 17, 2019. 1<br/></p>
<p>[35] Alexander Quinn Nichol and Prafulla Dhariwal. Improved<br/>denoising diffusion probabilistic models. In ICML, 2021. 2,<br/>3<br/></p>
<p>[36] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash<br/>Vahdat, and Anima Anandkumar. Diffusion models for ad-<br/>versarial purification. In ICML, 2022. 2, 7, 11<br/></p>
<p>[37] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-<br/>wongsa, and Supasorn Suwajanakorn. Diffusion autoen-<br/>coders: Toward a meaningful and decodable representation.<br/>In CVPR, 2022. 2, 4, 5, 7<br/></p>
<p>[38] Haonan Qiu, Chaowei Xiao, Lei Yang, Xinchen Yan,<br/>Honglak Lee, and Bo Li. Semanticadv: Generating adver-<br/>sarial examples via attribute-conditioned image editing. In<br/>ECCV, 2020. 1, 2, 3, 5<br/></p>
<p>[39] Sylvestre-Alvise Rebuffi, Sven Gowal, Dan A. Calian, Flo-<br/>rian Stimberg, Olivia Wiles, and Timothy Mann. Fixing<br/>data augmentation to improve adversarial robustness. arXiv<br/>preprint arXiv:2103.01946, 2021. 2<br/></p>
<p>[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,<br/>Patrick Esser, and Bjo&#776;rn Ommer. High-resolution image syn-<br/>thesis with latent diffusion models. In CVPR, 2022. 2<br/></p>
<p>[41] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-<br/>Net: Convolutional networks for biomedical image segmen-<br/>tation. In MICCAI, 2015. 3<br/></p>
<p>[42] ADAM SATARIANO. Police use of facial recognition is<br/>accepted by british court. The New York Times, 4, 2019. 1<br/></p>
<p>[43] Florian Schroff, Dmitry Kalenichenko, and James Philbin.<br/>FaceNet: A unified embedding for face recognition and clus-<br/>tering. In CVPR, 2015. 1<br/></p>
<p>[44] Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li,<br/>Haitao Zheng, and Ben Y Zhao. Fawkes: Protecting pri-<br/>vacy against unauthorized deep learning models. In 29th<br/>USENIX Security Symposium (USENIX Security 20), pages<br/>1589&#8211;1604, 2020. 2<br/></p>
<p>[45] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and<br/>Michael K Reiter. A general framework for adversarial ex-<br/>amples with objectives. ACM Transactions on Privacy and<br/>Security (TOPS), 22(3):1&#8211;30, 2019. 1, 2, 3<br/></p>
<p>[46] Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vi-<br/>gna. Portrait of a Privacy Invasion. Proceedings on Privacy<br/>Enhancing Technologies, 2015(1):41&#8211;60, 2015. 1<br/></p>
<p>[47] Maya Shwayder. Clearview ai&#8217;s facial-recognition app is a<br/>nightmare for stalking victims. Digital Trends, 2020. 1<br/></p>
<p>[48] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,<br/>and Surya Ganguli. Deep unsupervised learning using<br/>nonequilibrium thermodynamics. In ICML, 2015. 2, 3<br/></p>
<p>[49] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning<br/>structured output representation using deep conditional gen-<br/>erative models. In NeurIPS, 2015. 2<br/></p>
<p>[50] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-<br/>ing diffusion implicit models. In ICLR, 2021. 2, 3, 4<br/></p>
<p>[51] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-<br/>hishek Kumar, Stefano Ermon, and Ben Poole. Score-based<br/>generative modeling through stochastic differential equa-<br/>tions. arXiv preprint arXiv:2011.13456, 2020. 2<br/></p>
<p>[52] David Stutz, Matthias Hein, and Bernt Schiele. Disentan-<br/>gling adversarial robustness and generalization. In CVPR,<br/>2019. 2<br/></p>
<p>[53] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and<br/>Qifeng Chen. High-fidelity GAN inversion for image at-<br/>tribute editing. In CVPR, 2022. 7, 11<br/></p>
<p>[54] Zekai Wang, Tianyu Pang, Chao Du, Min Lin, Weiwei Liu,<br/>and Shuicheng Yan. Better diffusion models further improve<br/>adversarial training. arXiv preprint arXiv:2302.04638, 2023.<br/>2<br/></p>
<p>[55] Eric Wong and J Zico Kolter. Learning perturbation sets for<br/>robust machine learning. In ICLR, 2021. 2<br/></p>
<p>[56] Chaowei Xiao, Bo Li, Jun-Yan Zhu, Warren He, Mingyan<br/>Liu, and Dawn Song. Generating adversarial examples with<br/>adversarial networks. In IJCAI, 2018. 2<br/></p>
<p>[57] Weilin Xu, David Evans, and Yanjun Qi. Feature squeez-<br/>ing: Detecting adversarial examples in deep neural networks.<br/>arXiv preprint arXiv:1704.01155, 2017. 7<br/></p>
<p>[58] Xiao Yang, Yinpeng Dong, Tianyu Pang, Hang Su, Jun Zhu,<br/>Yuefeng Chen, and Hui Xue. Towards face encryption by<br/>generating adversarial identity masks. In CVPR, 2021. 1, 2,<br/>3, 5, 6, 11, 13, 14<br/></p>
<p>[59] Bangjie Yin, Wenxuan Wang, Taiping Yao, Junfeng Guo,<br/>Zelun Kong, Shouhong Ding, Jilin Li, and Cong Liu. Adv-<br/>Makeup: A New Imperceptible and Transferable Attack on<br/>Face Recognition. arXiv preprint arXiv:2105.03162, 2021.<br/>1, 2, 3<br/></p>
<p>[60] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,<br/>Gang Yu, and Nong Sang. BiSeNet: Bilateral segmenta-<br/>tion network for real-time semantic segmentation. In ECCV,<br/>2018. 5<br/></p>
<p>[61] Yaoyao Zhong and Weihong Deng. Towards transferable ad-<br/>versarial attack against deep face recognition. IEEE Transac-<br/>tions on Information Forensics and Security, 16:1452&#8211;1466,<br/>2020. 1, 3, 5</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Supplementary Material<br/>A. DiffProtect-fast<br/>A.1. Effect of t0<br/></p>
<p>In DiffProtect-fast, we run one generative step from the<br/>time stamp t0 to compute I(i)p , instead of running the whole<br/>generative process from t = T to t = 0 to accelerate attack<br/>generation. We show the effect of the choice of t0 in Fig. 9,<br/>where we use the same settings as in the ablation studies.<br/>We can observe that when we increase t0, ASR first in-<br/>creases and then decreases, while FID first decreases and<br/>then increases, which indicates the image quality also first<br/>increases and then decreases. This shows that choosing t0<br/>at the early or late stage of the generative process is less ef-<br/>fective. We set t0/T = 0.6, i.e., t0 = 0.6 &#183; T , as this ratio<br/>achieves the highest ASR and is also a local minimum for<br/>FID.<br/></p>
<p>0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9<br/>to/T<br/></p>
<p>35<br/></p>
<p>40<br/></p>
<p>45<br/></p>
<p>50<br/></p>
<p>55<br/></p>
<p>AS<br/>R <br/></p>
<p>(%<br/>)<br/></p>
<p>ASR<br/></p>
<p>31.75<br/>32.00<br/>32.25<br/>32.50<br/>32.75<br/>33.00<br/>33.25<br/>33.50<br/>33.75<br/></p>
<p>FI<br/>D<br/></p>
<p>FID<br/></p>
<p>Figure 9: Effect of t0/T ratio for DiffProtect-fast.<br/></p>
<p>A.2. Comparison with DiffProtect<br/>We provide quantitative comparisons between DiffPro-<br/></p>
<p>tect and DiffProtect-fast in Table 4. Compared to Diff-<br/>Protect, DiffProtect-fast achieves lower ASRs, and similar<br/>FIDs, but requires much less computation time. Note that<br/>DiffProtect-fast also outperforms the baseline methods as<br/>shown in Table 1 of the main paper. These results demon-<br/>strate that the attack acceleration strategy is very effective<br/>and we can use DiffProtect-fast to achieve a trade-off be-<br/>tween computation time and attack success rate. In addi-<br/>tion, by comparing the last two rows of Figs. 10 and 11, we<br/>can observe that the protected images generated by DiffPro-<br/>tect and DiffProtect-fast are visually similar.<br/></p>
<p>B. Additional Implementation Details<br/>B.1. Baselines<br/></p>
<p>For l&#8734; attacks such as PGD [33], MIM [7] and TIP-<br/>IM [58], the attack budget is set to be 16/255 with 100 at-<br/></p>
<p>tack iterations. For MIM, the decay factor &#181; is set to 1.0.<br/>For TIP-IM, the gamma value for the MMD loss is set to 0<br/>by default. For AMT-GAN [20], we use the default setting.<br/>B.2. Defense methods<br/></p>
<p>We evaluated our methods with four defense methods:<br/>feature squeezing [13], JPEG compression [9], median blur<br/>[30], and adversarial purification [36]. For feature squeez-<br/>ing, the bit depth is set to 3. For JPEG compression, the<br/>quality factor is set to 5. For the median blur, the kernel<br/>size is set to 7. For adversarial purification, we set the dif-<br/>fusion timestep to be 0.0075.<br/>B.3. GAN-based attack<br/></p>
<p>To have a fair comparison between GAN-based and<br/>Diffusion-based methods, we re-implement our method us-<br/>ing an HFGI [53] model, a state-of-the-art method that uses<br/>a high-fidelity GAN and can invert and edit high-resolution<br/>face images effectively. HFGI consists of an encoder E to<br/>encode the images and uses a generator G to edit the high-<br/>resolution face images. To craft the GAN-based attack, we<br/>first pass the image to the encoder to get the latent code<br/>z = E(I). Then we perturb z by minimizing Eq. (15) of<br/>the main paper to obtain the adversarial latent code zadv .<br/>Finally, we decode zadv to an adversarial image using the<br/>generator G. We set the attack budget to 0.1, with step size<br/>0.02 in 10 iterations.<br/></p>
<p>C. Additional Experimental Results<br/>We show the effect of attack iterations in Table 5, where<br/></p>
<p>we change the number of attack iterations N and the other<br/>settings are the same as in the ablation studies. We can ob-<br/>serve that as N increases, ASR also increases while FID<br/>stays relatively the same. We set N = 50 in our main ex-<br/>periments.<br/></p>
<p>D. User Study<br/>To further investigate the effect of different attack bud-<br/></p>
<p>gets for preserving facial identity, we conduct a user study<br/>using Amazon Mechanical Turk (MTurk), where we show<br/>the worker the original image I and the protected image Ip<br/>generated with different attack budget &#947;. For each pair of<br/>I and Ip, the worker is asked to provide a rating from 1 to<br/>10 of &#8220;how likely the two face images belong to the same<br/>person?&#8221; with 1 being &#8220;extremely unlikely&#8221; and 10 being<br/>&#8220;extremely likely&#8221;. We randomly selected 50 images from<br/>our CelebA-HQ test set for the user study and each pair of<br/>I and Ip is evaluated by ten MTurk workers. The average<br/>ratings are shown in Table 6. We chose &#947; = 0.03 since it<br/>achieves a high user rating as well as a high ASR (78.4%)<br/>and low FID (27.6). In practice, a user can choose the value<br/>of &#947; themselves based on their own preferences.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>Methods<br/>CelebA-HQ FFHQ<br/></p>
<p>ASR (%) &#8593; FID &#8595; Time (s) &#8595; ASR (%) &#8593; FID &#8595; Time (s) &#8595;IRSE50 IR152 MobileFace IRSE50 IR152 MobileFace<br/>DiffProtect-fast 68.4 49.8 72.1 26.7 18.9 50.8 47.6 47.0 26.4 18.3<br/>DiffProtect 78.4 60.3 77.9 27.6 36.4 57.7 54.3 52.9 26.1 35.9<br/></p>
<p>Table 4: Comparison between DiffProtect and DiffProtect-fast.<br/></p>
<p>N 2 5 10 20 50<br/>ASR (%) &#8593; 47.3 57.2 60.5 61.9 63.1<br/></p>
<p>FID &#8595; 32.4 33.3 33.4 33.6 33.7<br/>Table 5: Effect of attack iteration N .<br/></p>
<p>&#947; 0.005 0.01 0.02 0.03 0.05 0.1<br/>Rating &#8593; 8.9 8.7 7.9 7.0 5.5 4.7<br/></p>
<p>Table 6: Identity similarity ratings of different &#947;.<br/></p>
<p>E. More Visualization Results<br/>We provide more examples of protected face images<br/></p>
<p>in Figs. 10 and 11. We can observe that DiffProtect pro-<br/>duces good-looking protected images with natural and in-<br/>conspicuous changes to the input images, such as slight<br/>changes in facial expressions. It works well across genders,<br/>ages, and races, and in some cases even makes the images<br/>look more attractive. Compared to TIP-IM, the protected<br/>face images generated by DiffProtect have no obvious noise<br/>pattern as we only perturb the semantic codes and gener-<br/>ate the images through a conditional DDIM. Compared to<br/>AMT-GAN, DiffProtect can better preserve image styles<br/>and details and does not require training a target-specific<br/>model for each identity.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In<br/>pu<br/></p>
<p>t<br/>T<br/></p>
<p>IP<br/>-I<br/></p>
<p>M<br/>[5<br/></p>
<p>8]<br/>A<br/></p>
<p>M<br/>T-<br/></p>
<p>G<br/>A<br/></p>
<p>N<br/>[2<br/></p>
<p>0]<br/>D<br/></p>
<p>iff<br/>Pr<br/></p>
<p>ot<br/>ec<br/></p>
<p>t-<br/>fa<br/></p>
<p>st<br/>D<br/></p>
<p>iff<br/>Pr<br/></p>
<p>ot<br/>ec<br/></p>
<p>t<br/></p>
<p>Figure 10: Visualizations of the protected face images generated by different face encryption methods on CelebA-HQ.</p>

</div></div>
<div style="page-break-before:always; page-break-after:always"><div><p>In<br/>pu<br/></p>
<p>t<br/>T<br/></p>
<p>IP<br/>-I<br/></p>
<p>M<br/>[5<br/></p>
<p>8]<br/>A<br/></p>
<p>M<br/>T-<br/></p>
<p>G<br/>A<br/></p>
<p>N<br/>[2<br/></p>
<p>0]<br/>D<br/></p>
<p>iff<br/>Pr<br/></p>
<p>ot<br/>ec<br/></p>
<p>t-<br/>fa<br/></p>
<p>st<br/>D<br/></p>
<p>iff<br/>Pr<br/></p>
<p>ot<br/>ec<br/></p>
<p>t<br/></p>
<p>Figure 11: Visualizations of the protected face images generated by different face encryption methods on CelebA-HQ.</p>

</div></div>
</body></html>